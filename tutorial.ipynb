{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMlzpLac1GOz"
   },
   "source": [
    "# Welcome to the NS-Gym Tutorial!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40mnpJzZRcnA"
   },
   "source": [
    "The goal of this tutorial is to get you started using NS-Gym to make NS-MDPs.\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Learn the basic steps to create a NS-MDP using NS-Gym.\n",
    "2. Walkthrough a simple example how one might use NS-Gym to evaluate policy perfomacnce on NS-MDPs.\n",
    "3. Create our own custom NS-MDP using NS-Gym.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOLQIF0D1E9S",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Installation\n",
    "\n",
    "Typically we can pip install from github but for the purposes of this submission please refer to this read me for installation instructions: [README.md](README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rs16CKpJ1vaW"
   },
   "source": [
    "## Let's import the ns_gym package and other useful packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xm2LM8XE_bQo"
   },
   "outputs": [],
   "source": [
    "\n",
    "# This is our Wrapper\n",
    "import ns_gym as nsg\n",
    "\n",
    "# Of course we need to import gymnasium\n",
    "import gymnasium as gym\n",
    "\n",
    "# Other useful packages for analysis and visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from ns_gym.utils import type_mismatch_checker\n",
    "import copy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBQAJCFGEWev"
   },
   "source": [
    "# A simple motivating example: Non-stationary FrozenLake\n",
    "\n",
    "Let's take a look at as simple motivating case where we learn a deteministic value iteratoin policy on stationary frozen lake.\n",
    "\n",
    "Value iterattion is dynamic programming approach to solve for the expected long term reward of a cell given the optimal policy. For demonstration purposses we have an an implemation below. In this example this serves as our **decision-making agent**.\n",
    "\n",
    "For now let us assume this agent only has a decision making agent only has access to a planning module with no \"runtime monitor\" or \"model updater\" component. We will revisit how NS-Gym might simulate these two components later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XvReFePzFR04"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def value_iteration(env, gamma=0.9, theta=1e-6):\n",
    "    \"\"\"\n",
    "    Perform value iteration on a Gymnasium environment.\n",
    "\n",
    "    Args:\n",
    "        env: The Gymnasium environment (assumed to be FrozenLake).\n",
    "        gamma: The discount factor.\n",
    "        theta: A threshold for stopping the value iteration.\n",
    "\n",
    "    Returns:\n",
    "        policy: The optimal policy.\n",
    "        V: The value function for each state.\n",
    "    \"\"\"\n",
    "    # Initialize value function\n",
    "    V = np.zeros(env.unwrapped.observation_space.n)\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # Update each state's value\n",
    "        for state in range(env.unwrapped.observation_space.n):\n",
    "            # Compute the maximum expected value over all possible actions\n",
    "            v = V[state]\n",
    "            new_v = max(\n",
    "                sum(prob * (reward + gamma * V[next_state])\n",
    "                    for prob, next_state, reward, _ in env.unwrapped.P[state][action])\n",
    "                for action in range(4)\n",
    "            )\n",
    "            # Update the value function\n",
    "            V[state] = new_v\n",
    "            # Update the delta\n",
    "            delta = max(delta, abs(v - new_v))\n",
    "\n",
    "        # Check for convergence\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    # Extract the optimal policy\n",
    "    policy = np.zeros(env.unwrapped.observation_space.n, dtype=int)\n",
    "    for state in range(env.unwrapped    .observation_space.n):\n",
    "        # Select the action with the highest expected value\n",
    "        policy[state] = np.argmax([\n",
    "            sum(prob * (reward + gamma * V[next_state])\n",
    "                for prob, next_state, reward, _ in env.unwrapped.P[state][action])\n",
    "            for action in range(4)\n",
    "        ])\n",
    "\n",
    "    return policy, V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqihIDiUQn0R"
   },
   "source": [
    "## A quick intro to the standard Gymnasium API and workflow\n",
    "\n",
    "In Gymnasium, an environment represents a Markov Decision Process (MDP). It provides a framework where an agent interacts with an environment by taking actions, observing states, and receiving rewards. Here’s a breakdown of key components:\n",
    "\n",
    "**Environment** Object: This represents the MDP. It includes a set of states, a set of possible actions, and defines the rules for state transitions and rewards based on the agent's actions.\n",
    "\n",
    "**Observation**: Represents the information the agent receives about the current state. It may not always provide complete information about the true state, depending on the environment's design.\n",
    "\n",
    "**Info**: This is an optional dictionary that provides extra diagnostic information helpful for debugging or understanding the environment's behavior. It's not used directly for learning but can provide insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqw1FeuuUt5E"
   },
   "source": [
    "### Step 1) Initialization\n",
    "\n",
    "The environment is first initialized to set up the state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F_YH1p7rUtJG"
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1',render_mode=\"rgb_array\",is_slippery=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdRW-nVoVE3R"
   },
   "source": [
    "### Step 2) Interaction Loop\n",
    "\n",
    "The agent interacts with the environment in a loop:\n",
    "\n",
    "* Receives an observation of the current state.\n",
    "* Chooses an action based on the observation.\n",
    "* Executes the action using env.step(action), which returns:\n",
    "\n",
    "    * The next observation.\n",
    "    * A reward for the action taken.\n",
    "    * A done flag indicating if the agent has reached a terminal state.\n",
    "    * A truncated flag indicating if the episode has elapsed for a maximum number of steps.\n",
    "    * The info dictionary.\n",
    "\n",
    "If `done` is True, the environment is reset for a new episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHM4guU6Qm8i"
   },
   "outputs": [],
   "source": [
    "#Let's grab the stationary policy  -- our AGENT\n",
    "\n",
    "\n",
    "def run_episode(env, policy, visulalize=True,non_stationary_env=False):\n",
    "\n",
    "    # Reset environment\n",
    "    observation, info = env.reset()\n",
    "    observation, _ = type_mismatch_checker(observation)\n",
    "\n",
    "    # Set done and truncated flags to false\n",
    "    done = False\n",
    "    truncated = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    parameter_list = []\n",
    "\n",
    "    if non_stationary_env:\n",
    "        parameter_list.append(copy.copy(env.transition_prob))\n",
    "\n",
    "    while not done and not truncated:\n",
    "\n",
    "        ###############  Main agent-environment interaction #######\n",
    "        action = policy[observation]\n",
    "        observation, reward, done, truncated, info = env.step(action)\n",
    "        observation, reward = type_mismatch_checker(observation,reward)\n",
    "        episode_reward += reward\n",
    "\n",
    "\n",
    "        if non_stationary_env:\n",
    "            parameter_list.append(copy.copy(env.transition_prob))\n",
    "\n",
    "        #############################################################\n",
    "\n",
    "        if visulalize:\n",
    "            ####### Vizulaize agent in environment##########################\n",
    "            # Clear the previous output , for visualization\n",
    "            clear_output(wait=True)\n",
    "            output= env.render()\n",
    "            plt.imshow(output)\n",
    "            plt.axis('off')  # Hide axes\n",
    "            plt.show()\n",
    "            #############################################################\n",
    "            time.sleep(0.3)\n",
    "\n",
    "    return episode_reward,parameter_list\n",
    "\n",
    "stationary_policy, stationary_V = value_iteration(env.unwrapped)\n",
    "\n",
    "for i in range(5):\n",
    "    run_episode(env,stationary_policy)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzuvdnP8g2ZY"
   },
   "source": [
    "# **Integrating NS-Gym** -- what if at each decision epoch the environment becomes more slippery?\n",
    "\n",
    "We can capute the change in this environmental parameter with the NS-Gym package.\n",
    "\n",
    "The agent environment set up is essentially follows the same Gymnasium interface with few other considerations. There are finer details that we will go over for a bit but the general experimental pipeline is as follows:\n",
    "\n",
    "\n",
    "\n",
    "1. **Create Standard Environment**: Start by creating a typical Gymnasium environment.\n",
    "2. **Define Update Parameters**: Identify environmental parameters to be updated during the episode.\n",
    "3. **Map Schedulers & Update Functions**: Assign schedulers and update functions to each parameter.\n",
    "4. **Generate Non-Stationary Environment**: Use the NS-Gym wrapper to combine the standard environment with the parameter mappings, creating a non-stationary environment.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDTXy9_mW2NW"
   },
   "source": [
    "## Step 1) Create a standard Gymnasium environment\n",
    "\n",
    "We initialized the Gymnasium Frozen Lake environment above but for completeness we can do it again here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p5NChdNll6PF"
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1',render_mode=\"rgb_array\",max_episode_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMZ77GuvmGTl"
   },
   "source": [
    "## Step 2) Define Update Parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpWLg_9cW2NX"
   },
   "source": [
    "NS-Gym in at a high level lets users design non-stationary marchov decision processes by modifiing exposed paramters of some base Gymnasium environment.\n",
    "\n",
    "The table below lists all environments and set of observable parameters that can be tuned by NS-Gym to induce non-stationarity.\n",
    "\n",
    "| Env | What are the tunable parameters (AKA the hidden theta) |\n",
    "| --- | -------------------------------------------------------|\n",
    "| Acrobot|\"dt\",\"LINK_LENGTH_1\",\"LINK_LENGTH_2\",\"LINK_MASS_1\",\"LINK_MASS_2\",\"LINK_COM_POS_1\",\"LINK_COM_POS_2\",\"LINK_MOI\"|\n",
    "| Cartpole|\"gravity\",\"masscart\",\"masspole\",\"force_mag\",\"tau\",\"length\"|\n",
    "| Mountain Car | 'force','gravity'|\n",
    "| Pendulum |'dt', 'g','l', 'm',|\n",
    "| Frozen Lake |'P' (P is a probablity table that defines a categorical distribution for each state action pair)|\n",
    "| Cliff Walking |'P'|\n",
    "| Bridge | \"P\", \"left_side_prob\",\"right_side_prob\" |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PR0G2l5XmQx_"
   },
   "outputs": [],
   "source": [
    "####### Defining environmental parameters ############\n",
    "param = \"P\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeE5BwIFW2NX"
   },
   "source": [
    "\n",
    "\n",
    "## Set 3 Map Schedulers and Parameter Update Functions\n",
    "\n",
    "In addition to a wrapper, the two other essential components of the package are schedulers and pupdate functions.\n",
    "\n",
    "- **Schedulers:** Functions (really callables) that define **when** to update the value of an environment transition function. They simply return a boolean flag at time steps where the paramters need to updated.\n",
    "- **Update Function:** Defines **how** to update the value of a parameter or probability distribution. If the scheduler returns true, update the parameter accordingly.\n",
    "\n",
    "\n",
    "Seperating each component allows for greater flexibility and in designing experiments.\n",
    "\n",
    "Availble schedulers are found under `ns_gym/schedulers.py`. Parameter update functions are found under the `ns_gym/update_functions` directory.\n",
    "\n",
    "In this case we want the **frequency** of our updates to be at each MDP step. So let's import the `ContinuousScheduler`.\n",
    "\n",
    "We can decrease the probability of going in the direction by some constant using the `DistributionDecrmentUpdate`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0hHD4Kdbqoae"
   },
   "outputs": [],
   "source": [
    "from ns_gym.schedulers import ContinuousScheduler,DiscreteScheduler\n",
    "from ns_gym.update_functions import DistributionDecrementUpdate\n",
    "\n",
    "### Define the scheduler #######\n",
    "scheduler = ContinuousScheduler() #Update the slipperiness at each timestep\n",
    "\n",
    "#### Define the update function #####\n",
    "update_function = DistributionDecrementUpdate(scheduler=scheduler,k = 0.1) #Decrement the slipperiness by 0.1 at each timestep where the scheduler fires true\n",
    "\n",
    "# Map parameter to update function\n",
    "\n",
    "params = {param:update_function}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhqcGoJ2mx_F"
   },
   "source": [
    "## Step 4) Generate a Non-Stationary Environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rq2yJ4yOmwFG"
   },
   "source": [
    "NS-Gym provides wrappers for the Clasic control suite of Gymnasium  environments and three gridworld environments. At each time step these wrappers will modfify the value of the environment parameter thereby altering the transition function of the MDP. Aditionally, the wrapper can can control the level of notification available to decision making agent when there is a change to the tranition function.  \n",
    "\n",
    "The available wrappers are as follows:\n",
    "\n",
    "- The `NSClassicControlWrapper` is compatable with all environments in the classic control suite of environments.\n",
    "- The `NSCliffWalkingWrapper` augments the CliffWalking environment.\n",
    "- The `NSFrozenLakeWrapper` augments the FrozenLake environment.\n",
    "- The `NSBridgeWrapper` augments the Bridge environment.\n",
    "\n",
    "Wrappers are found under the `ns_gym.wrappers` subpackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zhoMgj3nwha0"
   },
   "outputs": [],
   "source": [
    "###### Import the frozen lake\n",
    "from ns_gym.wrappers import NSFrozenLakeWrapper\n",
    "\n",
    "ns_env = NSFrozenLakeWrapper(env,params, initial_prob_dist=[1,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pc7xUM36yNEx"
   },
   "source": [
    "# Observing our now stale policy on this non-stationary environment\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hpqwoFmePgz8"
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_parameter_over_time(parameter_values, final_reward):\n",
    "    \"\"\"\n",
    "    Plots the parameter values over time, color-coded by the final reward.\n",
    "\n",
    "    Args:\n",
    "        parameter_values (list or array): The parameter values over time.\n",
    "        final_reward (float): The final reward earned by the policy.\n",
    "    \"\"\"\n",
    "    time = np.arange(len(parameter_values))\n",
    "\n",
    "    # Create a color map based on the final reward\n",
    "    intended_direction_prob = [p[0] for p in parameter_values]\n",
    "    plt.plot(time, intended_direction_prob)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Transition Probability')\n",
    "    plt.title(f'Final Reward: {final_reward}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LVuo0WW_ydcJ"
   },
   "outputs": [],
   "source": [
    "for trial in range(5):\n",
    "    episode_reward,param_list = run_episode(ns_env,stationary_policy,non_stationary_env=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQxBRdEKYQbo"
   },
   "source": [
    "## One question we can answer using NS-Gym is how will different magnitudes of change affect the perfomance of our decision making agent?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AxkuLbGZYLnT"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_experiment(episode_runner,change_notification=False,delta_change_notification=False):\n",
    "    lookup_table = {}\n",
    "\n",
    "    for k in np.linspace(0,0.7,10):\n",
    "\n",
    "            ## Initialize the environement\n",
    "            env = gym.make('FrozenLake-v1',render_mode=\"rgb_array\",max_episode_steps=50)\n",
    "\n",
    "            ### Define the scheduler #######\n",
    "            # scheduler = ContinuousScheduler() #Update the slipperiness at each timestep\n",
    "            # scheduler = ContinuousScheduler(start =1,end=1 )\n",
    "            scheduler = DiscreteScheduler({1})\n",
    "            #### Define the update function #####\n",
    "            update_function = DistributionDecrementUpdate(scheduler=scheduler,k = k) #Decrement the slipperiness by 0.1 at each timestep where the scheduler fires true\n",
    "\n",
    "            # Map parameter to update function\n",
    "            params = {param:update_function}\n",
    "\n",
    "            # Wrap the environment\n",
    "            ns_env = NSFrozenLakeWrapper(env, params, change_notification,delta_change_notification,initial_prob_dist=[1,0,0])\n",
    "\n",
    "            # Define a list to keep track of trial rewards\n",
    "            trial_reward = []\n",
    "            for trial in range(100):\n",
    "                episode_reward,param_list = episode_runner(ns_env,stationary_policy,non_stationary_env=True,visulalize=False)\n",
    "                trial_reward.append(episode_reward)\n",
    "\n",
    "            lookup_table[k] = np.mean(trial_reward)\n",
    "    return lookup_table\n",
    "\n",
    "def plot_rate_of_change(lookup_tables):\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for name,lookup_table in lookup_tables.items():\n",
    "        plt.plot(lookup_table.keys(), lookup_table.values())\n",
    "\n",
    "    plt.title(\"Change in Policy Performance\")\n",
    "    plt.xlabel(\"Magnitude of Change in Transition Probability\")\n",
    "    plt.ylabel(\"Success Rate of the Agent\")\n",
    "    plt.legend(lookup_tables.keys())\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "lookup_table = run_experiment(run_episode)\n",
    "plot_rate_of_change({\"Stationary Policy\":lookup_table})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N01_eI9XdV-l"
   },
   "source": [
    " ### The is a trivial example and the result is to be expected, but what if our decision making agent had acess to two more modules?\n",
    "\n",
    "1. A runtime monitor component\n",
    "2. A model updater\n",
    "\n",
    "How would our policy perform is say between decions we had sufficient time to update our policy given the most up to date version of the environment (though no future versions)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DwjXEJsaW2NY"
   },
   "source": [
    "## Notification Levels and custom observation and reward types.\n",
    "\n",
    "A key feature of the NS-Gym library is its ability to manage\n",
    "the interaction between the environment and the decision-\n",
    "making agent.\n",
    "Users can configure notifications the agent receives about\n",
    "changes in the NS-MDP at three distinct levels:\n",
    "1. **Basic Notification:** The agent receives a boolean flag in\n",
    "dicating a change in an environment parameter.\n",
    "2. **Detailed Notification:** In addition to the boolean flag, the\n",
    "agent is informed of the magnitude of the change.\n",
    "3. **Full Environment Model:** Additionally, if the agent requires an environmental model for planning purposes (such\n",
    "as in Monte Carlo tree search), NS-Gym can provide a sta-\n",
    "tionary snapshot of the environment. This snapshot aligns\n",
    "with the basic or detailed notification settings configured by\n",
    "the user. If the user seeks a model without detailed notifi-\n",
    "cation, the planning environment is a stationary snapshot of\n",
    "the base environment. Conversely, if detailed notifications\n",
    "are enabled, the agent receives the most up-to-date version\n",
    "of the environment model (but not any future evolutions)\n",
    "\n",
    "To handle the different levels of notification NS-Gym has custom Obsevation and Reward types. The base Observation type is outlined below.\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Observation:\n",
    "    \"\"\"Observation dataclass type. This is the output of the step function in the environment.\n",
    "\n",
    "    Attributes:\n",
    "        state (Union[np.ndarray,int]): The state of the environment\n",
    "        env_change (Union[dict[str, bool],bool,None]): A dictionary of boolean flags indicating what param of the environment has changed.\n",
    "        delta_change (Union[dict[str,float],float]): The amount of change in the transition function of the environment\n",
    "        relative_time (Union[int,float]): The relative time of the observation since the start of the environment episode.\n",
    "    \"\"\"\n",
    "    state : Union[np.ndarray,int]\n",
    "    env_change: Union[dict[str, bool],None]\n",
    "    delta_change: Union[dict[str,float],float,None]\n",
    "    relative_time: Union[int,float]\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "So let's say that we now have a run time monitor and model updater component. We can simulate a detecting a anomally by way of the ns-gym notificaiton sytem.\n",
    "\n",
    "In this senerio our environment is still behaving in the same non-stationary way but we are now have a fully updated model of the environment, but no knowledge of future changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PwKaKTP0fZBc"
   },
   "outputs": [],
   "source": [
    "### Lets incorporate the simulation of a model updater and anomally detection\n",
    "### with ns_gym notification system\n",
    "\n",
    "### Set up how the environment changes as we have done previouslly\n",
    "env = gym.make('FrozenLake-v1',render_mode=\"rgb_array\",max_episode_steps=50)\n",
    "\n",
    "### Define the scheduler #######\n",
    "scheduler = ContinuousScheduler() #Update the slipperiness at each timestep\n",
    "\n",
    "#### Define the update function #####\n",
    "update_function = DistributionDecrementUpdate(scheduler=scheduler,k = 0.1) #Decrement the slipperiness by 0.1 at each timestep where the scheduler fires true\n",
    "\n",
    "# Map parameter to update function\n",
    "params = {param:update_function}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_hgJ8xxgEeU"
   },
   "source": [
    "For this scenerio we want two things:\n",
    "\n",
    "1. We \"notified\" that the environment changed so that we can retrain our model\n",
    "2. We need the most upt to date verision of the evironment to extract the policy.\n",
    "\n",
    "We can acheive this by setting the `change_notification` and `delta_change_notification` keywork parameters to `True` in our FrozenLake wrapper.\n",
    "\n",
    "By setting these wrapper parameters to true we have access the `env_change` and `delta_env` change fields of observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GklY3P7ygD0H"
   },
   "outputs": [],
   "source": [
    "# Wrap the environment\n",
    "ns_env = NSFrozenLakeWrapper(env,\n",
    "                             params,\n",
    "                             change_notification=True,\n",
    "                             delta_change_notification=True,\n",
    "                             initial_prob_dist=[1,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVereibYkSTH"
   },
   "source": [
    "Let's set up our new interaction loop this time updating our policy every time receive our change notification. Below I copy and pasted our agent-environment interaction loop from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sd7VqFwCkRJI"
   },
   "outputs": [],
   "source": [
    "def run_episode_with_notification(env, policy, visulalize=True,non_stationary_env=False):\n",
    "\n",
    "    # Reset environment\n",
    "    observation, info = env.reset()\n",
    "    observation, _ = type_mismatch_checker(observation)\n",
    "\n",
    "    # Set done and truncated flags to false\n",
    "    done = False\n",
    "    truncated = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    parameter_list = []\n",
    "\n",
    "    if non_stationary_env:\n",
    "        parameter_list.append(copy.copy(env.transition_prob))\n",
    "\n",
    "    while not done and not truncated:\n",
    "\n",
    "        ###############  Main agent-environment interaction #######\n",
    "        action = policy[observation]\n",
    "        observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        ### Check if the environment has changed ###\n",
    "        ## then update to a more uptodate policy)\n",
    "        if observation[\"env_change\"][\"P\"]:\n",
    "            policy,_ = value_iteration(env)\n",
    "\n",
    "\n",
    "\n",
    "        observation, reward = type_mismatch_checker(observation,reward)\n",
    "        episode_reward += reward\n",
    "\n",
    "\n",
    "        if non_stationary_env:\n",
    "            parameter_list.append(copy.copy(env.transition_prob))\n",
    "\n",
    "        #############################################################\n",
    "\n",
    "        if visulalize:\n",
    "            ####### Vizulaize agent in environment##########################\n",
    "            # Clear the previous output , for visualization\n",
    "            clear_output(wait=True)\n",
    "            output= env.render()\n",
    "            plt.imshow(output)\n",
    "            plt.axis('off')  # Hide axes\n",
    "            plt.show()\n",
    "            #############################################################\n",
    "            time.sleep(0.3)\n",
    "\n",
    "    return episode_reward,parameter_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ajlJw69HsAyj"
   },
   "outputs": [],
   "source": [
    "notification_lookup_table = run_experiment(run_episode_with_notification,change_notification=True,delta_change_notification=True)\n",
    "\n",
    "data = {\"Stationary Policy\":lookup_table,\n",
    "        \"Policy w/ Updates\":notification_lookup_table}\n",
    "\n",
    "plot_rate_of_change(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kp6asmia-giE"
   },
   "source": [
    "Although this is a minimal working example, this illustrates how one might use NS-Gym to benchmark policies under different conditions of non-stationarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eKdsCB0nW2Nc"
   },
   "source": [
    "# Constructing Custom NS-MDPs\n",
    "\n",
    "It is easy to construct custom NS-MDPS with NS-Gym to fit your use case via the implementation of custom scheduler and update functions.\n",
    "\n",
    "Lets say rather than a single discrete time environment parameter update we want the frequency in our updates to also be stochatic.\n",
    "\n",
    "Furthermore, we would like the slipperiness of the system to oscillate between a determenisitc one and a stochastic one.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAjZT-QqW2Nc"
   },
   "source": [
    "## Custom Scheduler\n",
    "One can easily implement their own custom scheduler. The scheduler just needs to be a callable that takes in the current time and outputs a boolean. It also needs to be a subclass of `ns_bench.base.Scheduler`. This sample custorm scheduler will \"fire\" with probability 0.25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I_6agpA1W2Nc"
   },
   "outputs": [],
   "source": [
    "### Custom Scheduler just needs to be a callable that takes in the current time and outputs a boolean. It also needs to be a subclass of ns_bench.base.Scheduler\n",
    "\n",
    "# Base classes are found here\n",
    "import ns_gym.base as base\n",
    "\n",
    "class StochasticScheduler(base.Scheduler):\n",
    "    def __init__(self,start=0,end=np.inf) -> None:\n",
    "        super().__init__(start,end)\n",
    "\n",
    "    def __call__(self,t):\n",
    "        \"\"\"This is where we define when the schedulers should fire.\n",
    "\n",
    "        Args:\n",
    "            t (Union[int,float]): The current time step\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the scheduler should fire, False otherwise\n",
    "        \"\"\"\n",
    "        return np.random.choice([True,False],p=[0.25,0.75])\n",
    "\n",
    "\n",
    "\n",
    "stochastic_scheduler = StochasticScheduler()\n",
    "\n",
    "## We can test it here\n",
    "\n",
    "count_true = 0\n",
    "\n",
    "for i in range(300):\n",
    "    if stochastic_scheduler(i):\n",
    "        count_true +=1\n",
    "\n",
    "print(f\"The scheduler fires {count_true/300} percent of the time\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVvxSScSW2Nd"
   },
   "source": [
    "## Custom Update Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WASnwroW2Nd"
   },
   "source": [
    "We can also define a custome update function. We can define a parameter update function as a callable class. The the parameter update function is sa subclass of `ns_gym.base.UpdateFn` if its a scalar parameter update. If we are modifying a probability `ns_gym.base.UpdateDistributionFn`. We initialize the update function with its associated scheduler then all we need to implement it the `update` method. The update funcition will only \"fire\" when the scheduler returns true. If there is no update, the `ns_gym.base.UpdateFn` will return the parameter with no change.\n",
    "\n",
    "All update functions return a three tuple `(param, update_bool, parameter_update_amount)`\n",
    "\n",
    "```\n",
    "    Returns:\n",
    "            Any: The updated parameter\n",
    "            bool: Boolean flag indicating whether the parameter was updated or not\n",
    "            float: The amount of change in the parameter\n",
    "```\n",
    "\n",
    "This custom update fuction will return a determensistic function or stochastic distribution depending on the current parameter value.\n",
    "\n",
    "It's important the note that the first element of a transition probability array always corresonds to the intended outcome of an action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kdb83abhW2Nd"
   },
   "outputs": [],
   "source": [
    "\n",
    "class OscillatingParameterUpdater(base.UpdateDistributionFn):\n",
    "    def __init__(self,scheduler) -> None:\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__(scheduler)\n",
    "\n",
    "    def _update(self,param,t):\n",
    "\n",
    "        if param[0] == 1:\n",
    "            param[0] = 0.4\n",
    "            for i in range(1,len(param)):\n",
    "                param[i] = (1-param[0])/(len(param)-1)\n",
    "        else:\n",
    "            param[0] = 1\n",
    "            for i in range(1,len(param)):\n",
    "                param[i] = 0\n",
    "\n",
    "        return param\n",
    "\n",
    "\n",
    "oscillating_updater = OscillatingParameterUpdater(stochastic_scheduler)\n",
    "\n",
    "### We can test the out put of out function here;\n",
    "\n",
    "test_param = [1,0,0]\n",
    "\n",
    "for t in range(20):\n",
    "    test_param,change_notif,delta_change = oscillating_updater(test_param,t)\n",
    "\n",
    "    if change_notif:\n",
    "        print(\"The parameter was updated!\")\n",
    "        print(f\"The new parameter value is: {test_param}\")\n",
    "        print(f\"The change in distribution (Wassestein distance): {delta_change}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z4pHwmzLNsYK"
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1',render_mode=\"rgb_array\",max_episode_steps=50)\n",
    "\n",
    "param = \"P\"\n",
    "\n",
    "custom_ns_mdp_parameters = {param:oscillating_updater}\n",
    "\n",
    "custom_ns_env = NSFrozenLakeWrapper(env,\n",
    "                             custom_ns_mdp_parameters,\n",
    "                             change_notification=True,\n",
    "                             delta_change_notification=True,\n",
    "                             initial_prob_dist=[1,0,0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ie0nHN9LXbS4"
   },
   "source": [
    " # Decision Making Algorithms Included in NS-Gym\n",
    "\n",
    "NS-Gym includes several decision making algorithms found under the `ns_gym.benchmark_algorithm` subpackage. For illustrative purposes we can test a vanilla Monte Carlo Tree Search (MCTS)  on our custom environment. MCTS needs a model of the environment to do search. We can give the search tree model that matches our notification levels by calling `ns_env.get_planning_env()`. If `delta_change_notification` is set to `True` the tree search algorithm has access to the most up to date version of the environment (though no future evolutions).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgN1vWubW2Nc"
   },
   "outputs": [],
   "source": [
    "from ns_gym.benchmark_algorithms import MCTS\n",
    "\n",
    "reward_list = []\n",
    "for i in range(10):\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    truncated = False\n",
    "    obs,_  = custom_ns_env.reset()\n",
    "\n",
    "    while not done and not truncated:\n",
    "        planning_env = custom_ns_env.get_planning_env()\n",
    "        agent = MCTS(planning_env,obs,d=100,m=100,c=1.44,gamma=0.99)\n",
    "        action,_ = agent.search()\n",
    "        obs, reward, done, truncated, info = custom_ns_env.step(action)\n",
    "        obs, reward = type_mismatch_checker(obs,reward)\n",
    "        episode_reward += reward\n",
    "\n",
    "    print(f\"Episode {i} reward: {episode_reward}\")\n",
    "    reward_list.append(episode_reward)\n",
    "\n",
    "\n",
    "print(f\"Success Rate for MCTS Agent on Custom Env: {np.mean(reward_list) }\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S1mpjg0OZkS6"
   },
   "outputs": [],
   "source": [
    "reward_list = []\n",
    "for i in range(100):\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    truncated = False\n",
    "    obs,_  = custom_ns_env.reset()\n",
    "    obs,_  = type_mismatch_checker(obs)\n",
    "\n",
    "    while not done and not truncated:\n",
    "        action = stationary_policy[obs]\n",
    "        obs, reward, done, truncated, info = custom_ns_env.step(action)\n",
    "        obs, reward = type_mismatch_checker(obs,reward)\n",
    "        episode_reward += reward\n",
    "\n",
    "    reward_list.append(episode_reward)\n",
    "\n",
    "\n",
    "print(f\"Success Rate for Stationary Agent on Custom Env: {np.mean(reward_list) }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The NS-Gym Eval Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to NS-MDP design, NS-Gym’s flexible API lets the programmer asses the ”difficulty” of the non-stationary MDPs via a simple function call from the NS-Gym’s eval module.  We include two baseline metrics. The first is an ensemble performance metric which computes the mean reward accross $N$ policies and $\\Pi$ policeis. For stochastic environments, we also provide a policy-agnostic metric, the PAMCTS-Bound. The PAMCTS-Bound measures the maximum difference in transition functions between two MDPs, $M_0$ and $M_1$.\n",
    "\n",
    "Below is an example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider designing a non-stationary version of the ”Pendulum” environment, where at each MDP time step, the mass of the pendulum increases by 0.01 units. This environment can be constructed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\")\n",
    "tunable_param = \"m\"\n",
    "scheduler = ContinuousScheduler()\n",
    "update_fn = nsg.update_functions.IncrementUpdate(scheduler=scheduler,k=0.01)\n",
    "param_map = {tunable_param:update_fn}\n",
    "ns_env = nsg.wrappers.NSClassicControlWrapper(env,param_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ensemble evalutar can be simply called as so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ns_gym.evaluate.metrics import EnsembleMetric\n",
    "evaluator = EnsembleMetric()\n",
    "ensemble_reward, perfomance_dict = evaluator.evaluate(ns_env,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating Contextual MDP with NS-Gym\n",
    "\n",
    "Contextual MDPs (C-MDPs) are a related modeling paradigm to NS-MDPs and can easilly be implemented in NS-Gym. Below is an example of how we can define a C-MDP over a specified context and evaluate it using a DQN model from the `Stable-Baseline3` library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ipywidgets --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ns_gym.context_switching import make_env_with_context, calculate_generalized_performance\n",
    "from functools import partial\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3 import DQN\n",
    "# Let's define the k source contexts to train on \n",
    "SOURCE_CONTEXTS = np.array([1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0])\n",
    "\n",
    "# define fully context range\n",
    "\n",
    "target_context_min = 0.1\n",
    "target_context_max = 10.0\n",
    "num_target_contexts = 100\n",
    "\n",
    "current_context_range = np.linspace(target_context_min,target_context_max,num_target_contexts)\n",
    "\n",
    "env_name = \"CartPole-v1\"\n",
    "context_param = \"masspole\"\n",
    "\n",
    "trained_agents = []\n",
    "make_env_partial_fn = partial(make_env_with_context, env_name=env_name,context_parameter_name=context_param)\n",
    "\n",
    "for i, s_ctx in enumerate(SOURCE_CONTEXTS):\n",
    "    train_env = make_vec_env(lambda: make_env_partial_fn(context_value=s_ctx, seed=i), n_envs=1)\n",
    "    model = DQN(\n",
    "        \"MlpPolicy\", \n",
    "        train_env, \n",
    "        verbose=0,\n",
    "        tensorboard_log=None,\n",
    "        device=\"auto\",\n",
    "        seed=i\n",
    "    )\n",
    "\n",
    "\n",
    "    model.learn(total_timesteps=3000, progress_bar=False)\n",
    "    trained_agents.append(model)\n",
    "    train_env.close()\n",
    "\n",
    "\n",
    "\n",
    "    (U_raw, peaks_raw, overall_raw, sem_raw,\n",
    "     U_norm, peaks_norm, overall_norm, sem_norm, \n",
    "     norm_params)  = calculate_generalized_performance(\n",
    "            trained_agents,\n",
    "            SOURCE_CONTEXTS,\n",
    "            make_env_partial_fn,\n",
    "            num_episodes_per_eval_context=25,\n",
    "            evaluation_target_context_range=current_context_range,\n",
    "            normalize=True\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Results Summary ---\")\n",
    "print(f\"Shape of Performance Matrix U (Raw): {U_raw.shape}\")\n",
    "if U_norm is not None:\n",
    "    print(f\"Shape of Performance Matrix U (Normalized): {U_norm.shape}\")\n",
    "\n",
    "print(\"\\nPeak Performance for each Policy (Raw):\")\n",
    "for i, peak_p in enumerate(peaks_raw):\n",
    "    print(f\"  Agent trained on Ctx {SOURCE_CONTEXTS[i]:.2f}: Max Reward = {peak_p:.2f}\")\n",
    "if peaks_norm is not None:\n",
    "    print(\"\\nPeak Performance for each Policy (Normalized):\")\n",
    "    for i, peak_p in enumerate(peaks_norm):\n",
    "        print(f\"  Agent trained on Ctx {SOURCE_CONTEXTS[i]:.2f}: Max Reward = {peak_p:.2f}\")\n",
    "    \n",
    "print(f\"\\nOverall System Generalized Performance: Mean = {overall_raw:.2f}, SEM = {sem_raw:.4f}\")\n",
    "\n",
    "print(f\"Overall System Generalized Performance: Mean = {overall_norm:.2f}, SEM = {sem_norm:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
