{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7b277a0",
   "metadata": {},
   "source": [
    "# PPO Tests Across NS-Gym Environments\n",
    "\n",
    "This is a testground for PPO on multiple environments.\n",
    "\n",
    "Tested Environments:\n",
    "- FrozenLake-v1\n",
    "- CliffWalking-v1\n",
    "- CartPole-v1\n",
    "- Acrobot-v1\n",
    "- MountainCar-v0\n",
    "- MountainCarContinuous-v0\n",
    "- Pendulum-v1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "788db3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ns_gym.schedulers import ContinuousScheduler\n",
    "from ns_gym.update_functions import IncrementUpdate, DistributionDecrementUpdate\n",
    "from ns_gym.wrappers import NSClassicControlWrapper, NSFrozenLakeWrapper, NSCliffWalkingWrapper\n",
    "from ns_gym.benchmark_algorithms import PPO, PPOActor, PPOCritic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d5b8a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving weights to: ./ppo_test_weights\n",
      "Episodes per environment: 30\n"
     ]
    }
   ],
   "source": [
    "# Runtime knobs\n",
    "MAX_EPISODES_PER_ENV = 30\n",
    "WEIGHTS_DIR = \"./ppo_test_weights\"\n",
    "os.makedirs(WEIGHTS_DIR, exist_ok=True)\n",
    "\n",
    "COMMON_CONFIG = {\n",
    "    \"batch_size\": 2048,\n",
    "    \"minibatch_size\": 64,\n",
    "    \"n_epochs\": 10,\n",
    "    \"hidden_size\": 64,\n",
    "    \"gamma\": 0.99,\n",
    "    \"lamb\": 0.95,\n",
    "    \"device\": \"cpu\",\n",
    "    \"lr_policy\": 3e-4,\n",
    "    \"lr_critic\": 4e-4,\n",
    "    \"max_grad_norm\": 0.5,\n",
    "    \"clip_val\": 0.2,\n",
    "    \"ent_weight\": 0.0,\n",
    "}\n",
    "\n",
    "print(f\"Saving weights to: {WEIGHTS_DIR}\")\n",
    "print(f\"Episodes per environment: {MAX_EPISODES_PER_ENV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d81e36e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_frozenlake_env(render_mode=None):\n",
    "    env_kwargs = {\n",
    "        \"is_slippery\": False,\n",
    "        \"max_episode_steps\": 100,\n",
    "    }\n",
    "    if render_mode is not None:\n",
    "        env_kwargs[\"render_mode\"] = render_mode\n",
    "\n",
    "    env = gym.make(\"FrozenLake-v1\", **env_kwargs)\n",
    "    scheduler = ContinuousScheduler()\n",
    "    update_fn = DistributionDecrementUpdate(scheduler=scheduler, k=0.01)\n",
    "    return NSFrozenLakeWrapper(\n",
    "        env,\n",
    "        {\"P\": update_fn},\n",
    "        change_notification=True,\n",
    "        delta_change_notification=True,\n",
    "        initial_prob_dist=[1, 0, 0],\n",
    "    )\n",
    "\n",
    "\n",
    "def make_cliffwalking_env():\n",
    "    env = gym.make(\"CliffWalking-v1\", max_episode_steps=200)\n",
    "    scheduler = ContinuousScheduler()\n",
    "    update_fn = DistributionDecrementUpdate(scheduler=scheduler, k=0.01)\n",
    "    return NSCliffWalkingWrapper(\n",
    "        env,\n",
    "        {\"P\": update_fn},\n",
    "        change_notification=True,\n",
    "        delta_change_notification=True,\n",
    "        initial_prob_dist=[1, 0, 0, 0],\n",
    "    )\n",
    "\n",
    "\n",
    "def make_classic_control_env(env_id, tunable_param, k):\n",
    "    env = gym.make(env_id)\n",
    "    scheduler = ContinuousScheduler()\n",
    "    update_fn = IncrementUpdate(scheduler=scheduler, k=k)\n",
    "    return NSClassicControlWrapper(\n",
    "        env,\n",
    "        {tunable_param: update_fn},\n",
    "        change_notification=True,\n",
    "        delta_change_notification=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e28e44be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FrozenLake-v1',\n",
       " 'CliffWalking-v1',\n",
       " 'CartPole-v1',\n",
       " 'Acrobot-v1',\n",
       " 'MountainCar-v0',\n",
       " 'MountainCarContinuous-v0',\n",
       " 'Pendulum-v1']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENV_SPECS = [\n",
    "    {\n",
    "        \"name\": \"FrozenLake-v1\",\n",
    "        \"make_env\": make_frozenlake_env,\n",
    "        \"max_steps\": 100,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"CliffWalking-v1\",\n",
    "        \"make_env\": make_cliffwalking_env,\n",
    "        \"max_steps\": 200,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"CartPole-v1\",\n",
    "        \"make_env\": lambda: make_classic_control_env(\"CartPole-v1\", \"masspole\", 1e-3),\n",
    "        \"max_steps\": 500,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Acrobot-v1\",\n",
    "        \"make_env\": lambda: make_classic_control_env(\"Acrobot-v1\", \"LINK_MASS_2\", 1e-3),\n",
    "        \"max_steps\": 500,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"MountainCar-v0\",\n",
    "        \"make_env\": lambda: make_classic_control_env(\"MountainCar-v0\", \"force\", 5e-5),\n",
    "        \"max_steps\": 200,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"MountainCarContinuous-v0\",\n",
    "        \"make_env\": lambda: make_classic_control_env(\"MountainCarContinuous-v0\", \"power\", 5e-5),\n",
    "        \"max_steps\": 999,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Pendulum-v1\",\n",
    "        \"make_env\": lambda: make_classic_control_env(\"Pendulum-v1\", \"m\", 1e-3),\n",
    "        \"max_steps\": 200,\n",
    "    },\n",
    "]\n",
    "\n",
    "[spec[\"name\"] for spec in ENV_SPECS]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98ac7373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_state_dim_from_obs(obs):\n",
    "    # NS-wrappers return dict, but PPO only wants the \"state\" part.\n",
    "    if isinstance(obs, dict) and \"state\" in obs:\n",
    "        obs = obs[\"state\"]\n",
    "\n",
    "    obs_arr = np.asarray(obs, dtype=np.float32)\n",
    "    if obs_arr.ndim == 0:\n",
    "        return 1\n",
    "    return int(obs_arr.reshape(-1).shape[0])\n",
    "\n",
    "\n",
    "def infer_state_dim_from_space(observation_space):\n",
    "    # Fallback path if reset-based inference is unavailable.\n",
    "    if isinstance(observation_space, gym.spaces.Dict) and \"state\" in observation_space.spaces:\n",
    "        return infer_state_dim_from_space(observation_space.spaces[\"state\"])\n",
    "\n",
    "    if hasattr(observation_space, \"shape\") and observation_space.shape is not None and len(observation_space.shape) > 0:\n",
    "        return int(np.prod(observation_space.shape))\n",
    "\n",
    "    return 1\n",
    "\n",
    "\n",
    "def infer_action_dim_and_type(action_space):\n",
    "    if hasattr(action_space, \"n\"):\n",
    "        return int(action_space.n), True\n",
    "    return int(np.prod(action_space.shape)), False\n",
    "\n",
    "\n",
    "def train_single_env(spec, max_episodes=MAX_EPISODES_PER_ENV):\n",
    "    env = spec[\"make_env\"]()\n",
    "\n",
    "    # Prefer runtime observation-based inference to avoid Dict-space mismatches.\n",
    "    sample_obs, _ = env.reset(seed=0)\n",
    "    s_dim = infer_state_dim_from_obs(sample_obs)\n",
    "    if s_dim <= 0:\n",
    "        s_dim = infer_state_dim_from_space(env.observation_space)\n",
    "\n",
    "    a_dim, is_discrete = infer_action_dim_and_type(env.action_space)\n",
    "\n",
    "    actor = PPOActor(\n",
    "        s_dim=s_dim,\n",
    "        a_dim=a_dim,\n",
    "        hidden_size=COMMON_CONFIG[\"hidden_size\"],\n",
    "        is_discrete=is_discrete,\n",
    "    )\n",
    "    critic = PPOCritic(s_dim=s_dim, hidden_size=COMMON_CONFIG[\"hidden_size\"])\n",
    "    agent = PPO(\n",
    "        actor,\n",
    "        critic,\n",
    "        lr_policy=COMMON_CONFIG[\"lr_policy\"],\n",
    "        lr_critic=COMMON_CONFIG[\"lr_critic\"],\n",
    "        clip_val=COMMON_CONFIG[\"clip_val\"],\n",
    "    )\n",
    "\n",
    "    config = dict(COMMON_CONFIG)\n",
    "    config.update(\n",
    "        {\n",
    "            \"env_name\": spec[\"name\"].replace(\"/\", \"_\"),\n",
    "            \"max_episodes\": max_episodes,\n",
    "            \"max_steps\": spec[\"max_steps\"],\n",
    "            \"save_path\": WEIGHTS_DIR + \"/\",\n",
    "            \"s_dim\": s_dim,\n",
    "            \"a_dim\": a_dim,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    best_reward = agent.train_ppo(env, config)\n",
    "    env.close()\n",
    "\n",
    "    return {\n",
    "        \"env\": spec[\"name\"],\n",
    "        \"is_discrete_action\": is_discrete,\n",
    "        \"s_dim\": s_dim,\n",
    "        \"a_dim\": a_dim,\n",
    "        \"best_reward\": float(best_reward),\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06ed7c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Training PPO on FrozenLake-v1 =====\n",
      "[Episode    0] reward = 0.0, mean_100 = 0.0, pg_loss = -0.098, v_loss = 0.039\n",
      "[Episode    1] reward = 0.0, mean_100 = 0.0, pg_loss = -0.024, v_loss = 0.001\n",
      "[Episode    2] reward = 0.0, mean_100 = 0.0, pg_loss = -0.052, v_loss = 0.001\n",
      "[Episode    3] reward = 0.0, mean_100 = 0.0, pg_loss = -0.017, v_loss = 0.001\n",
      "[Episode    4] reward = 0.0, mean_100 = 0.0, pg_loss = -0.066, v_loss = 0.001\n",
      "[Episode    5] reward = 0.0, mean_100 = 0.0, pg_loss = -0.059, v_loss = 0.000\n",
      "[Episode    6] reward = 0.0, mean_100 = 0.0, pg_loss = -0.007, v_loss = 0.000\n",
      "[Episode    7] reward = 0.0, mean_100 = 0.0, pg_loss = -0.023, v_loss = 0.000\n",
      "[Episode    8] reward = 0.0, mean_100 = 0.0, pg_loss = -0.019, v_loss = 0.000\n",
      "[Episode    9] reward = 0.0, mean_100 = 0.0, pg_loss = -0.036, v_loss = 0.000\n",
      "[Episode   10] reward = 0.0, mean_100 = 0.0, pg_loss = -0.042, v_loss = 0.000\n",
      "[Episode   11] reward = 0.0, mean_100 = 0.0, pg_loss = -0.076, v_loss = 0.000\n",
      "[Episode   12] reward = 0.0, mean_100 = 0.0, pg_loss = -0.048, v_loss = 0.000\n",
      "[Episode   13] reward = 0.0, mean_100 = 0.0, pg_loss = -0.019, v_loss = 0.000\n",
      "[Episode   14] reward = 0.0, mean_100 = 0.0, pg_loss = -0.029, v_loss = 0.000\n",
      "[Episode   15] reward = 0.0, mean_100 = 0.0, pg_loss = -0.039, v_loss = 0.000\n",
      "[Episode   16] reward = 0.0, mean_100 = 0.0, pg_loss = -0.034, v_loss = 0.000\n",
      "[Episode   17] reward = 0.0, mean_100 = 0.0, pg_loss = -0.019, v_loss = 0.000\n",
      "[Episode   18] reward = 0.0, mean_100 = 0.0, pg_loss = -0.031, v_loss = 0.000\n",
      "[Episode   19] reward = 0.0, mean_100 = 0.0, pg_loss = -0.036, v_loss = 0.000\n",
      "[Episode   20] reward = 0.0, mean_100 = 0.0, pg_loss = -0.010, v_loss = 0.000\n",
      "[Episode   21] reward = 0.0, mean_100 = 0.0, pg_loss = -0.042, v_loss = 0.000\n",
      "[Episode   22] reward = 0.0, mean_100 = 0.0, pg_loss = -0.036, v_loss = 0.000\n",
      "[Episode   23] reward = 0.0, mean_100 = 0.0, pg_loss = -0.052, v_loss = 0.000\n",
      "[Episode   24] reward = 0.0, mean_100 = 0.0, pg_loss = -0.025, v_loss = 0.000\n",
      "[Episode   25] reward = 0.0, mean_100 = 0.0, pg_loss = -0.032, v_loss = 0.000\n",
      "[Episode   26] reward = 0.0, mean_100 = 0.0, pg_loss = -0.037, v_loss = 0.000\n",
      "[Episode   27] reward = 0.0, mean_100 = 0.0, pg_loss = -0.057, v_loss = 0.000\n",
      "[Episode   28] reward = 0.0, mean_100 = 0.0, pg_loss = -0.066, v_loss = 0.000\n",
      "[Episode   29] reward = 0.0, mean_100 = 0.0, pg_loss = -0.039, v_loss = 0.000\n",
      "\n",
      "===== Training PPO on CliffWalking-v1 =====\n",
      "[Episode    0] reward = -1190.0, mean_100 = -1190.0, pg_loss = 0.054, v_loss = 7179.251\n",
      "[Episode    1] reward = -3566.0, mean_100 = -2378.0, pg_loss = 0.256, v_loss = 62699.145\n",
      "[Episode    2] reward = -3170.0, mean_100 = -2642.0, pg_loss = -0.181, v_loss = 22553.262\n",
      "[Episode    3] reward = -3170.0, mean_100 = -2774.0, pg_loss = -0.117, v_loss = 28609.369\n",
      "[Episode    4] reward = -1784.0, mean_100 = -2576.0, pg_loss = -0.439, v_loss = 4492.715\n",
      "[Episode    5] reward = -1883.0, mean_100 = -2460.5, pg_loss = 0.020, v_loss = 14715.211\n",
      "[Episode    6] reward = -1586.0, mean_100 = -2335.6, pg_loss = 0.021, v_loss = 11061.890\n",
      "[Episode    7] reward = -2180.0, mean_100 = -2316.1, pg_loss = -0.714, v_loss = 5475.516\n",
      "[Episode    8] reward = -1685.0, mean_100 = -2246.0, pg_loss = 0.203, v_loss = 12961.026\n",
      "[Episode    9] reward = -1982.0, mean_100 = -2219.6, pg_loss = -0.048, v_loss = 17076.906\n",
      "[Episode   10] reward = -1784.0, mean_100 = -2180.0, pg_loss = -0.337, v_loss = 7688.777\n",
      "[Episode   11] reward = -1982.0, mean_100 = -2163.5, pg_loss = -0.352, v_loss = 10262.413\n",
      "[Episode   12] reward = -1784.0, mean_100 = -2134.3, pg_loss = -0.195, v_loss = 10374.200\n",
      "[Episode   13] reward = -1982.0, mean_100 = -2123.4, pg_loss = 0.492, v_loss = 24182.461\n",
      "[Episode   14] reward = -2873.0, mean_100 = -2173.4, pg_loss = -0.195, v_loss = 18354.449\n",
      "[Episode   15] reward = -3170.0, mean_100 = -2235.7, pg_loss = -0.082, v_loss = 25284.359\n",
      "[Episode   16] reward = -1190.0, mean_100 = -2174.2, pg_loss = 0.121, v_loss = 4454.075\n",
      "[Episode   17] reward = -3368.0, mean_100 = -2240.5, pg_loss = -0.135, v_loss = 33276.410\n",
      "[Episode   18] reward = -2081.0, mean_100 = -2232.1, pg_loss = 0.067, v_loss = 17414.420\n",
      "[Episode   19] reward = -2576.0, mean_100 = -2249.3, pg_loss = -0.325, v_loss = 12492.227\n",
      "[Episode   20] reward = -2180.0, mean_100 = -2246.0, pg_loss = 0.559, v_loss = 73553.305\n",
      "[Episode   21] reward = -1784.0, mean_100 = -2225.0, pg_loss = -0.010, v_loss = 14990.522\n",
      "[Episode   22] reward = -2180.0, mean_100 = -2223.0, pg_loss = 0.067, v_loss = 22459.254\n",
      "[Episode   23] reward = -2180.0, mean_100 = -2221.2, pg_loss = -0.185, v_loss = 17646.689\n",
      "[Episode   24] reward = -1685.0, mean_100 = -2199.8, pg_loss = 0.234, v_loss = 20465.598\n",
      "[Episode   25] reward = -1190.0, mean_100 = -2161.0, pg_loss = -0.315, v_loss = 867.069\n",
      "[Episode   26] reward = -2081.0, mean_100 = -2158.0, pg_loss = 0.494, v_loss = 33115.363\n",
      "[Episode   27] reward = -3071.0, mean_100 = -2190.6, pg_loss = -0.161, v_loss = 41791.633\n",
      "[Episode   28] reward = -1685.0, mean_100 = -2173.2, pg_loss = -0.363, v_loss = 4910.962\n",
      "[Episode   29] reward = -1586.0, mean_100 = -2153.6, pg_loss = -0.529, v_loss = 3276.016\n",
      "\n",
      "===== Training PPO on CartPole-v1 =====\n",
      "[Episode    0] reward = 30.0, mean_100 = 30.0, pg_loss = -0.006, v_loss = 50.562\n",
      "[Episode    1] reward = 19.0, mean_100 = 24.5, pg_loss = -0.002, v_loss = 30.653\n",
      "[Episode    2] reward = 19.0, mean_100 = 22.7, pg_loss = -0.003, v_loss = 30.418\n",
      "[Episode    3] reward = 14.0, mean_100 = 20.5, pg_loss = -0.006, v_loss = 20.616\n",
      "[Episode    4] reward = 23.0, mean_100 = 21.0, pg_loss = -0.001, v_loss = 37.846\n",
      "[Episode    5] reward = 15.0, mean_100 = 20.0, pg_loss = -0.004, v_loss = 22.800\n",
      "[Episode    6] reward = 12.0, mean_100 = 18.9, pg_loss = -0.005, v_loss = 16.828\n",
      "[Episode    7] reward = 26.0, mean_100 = 19.8, pg_loss = -0.012, v_loss = 44.609\n",
      "[Episode    8] reward = 17.0, mean_100 = 19.4, pg_loss = -0.009, v_loss = 26.307\n",
      "[Episode    9] reward = 20.0, mean_100 = 19.5, pg_loss = -0.000, v_loss = 32.362\n",
      "[Episode   10] reward = 20.0, mean_100 = 19.5, pg_loss = -0.024, v_loss = 33.919\n",
      "[Episode   11] reward = 13.0, mean_100 = 19.0, pg_loss = -0.010, v_loss = 17.528\n",
      "[Episode   12] reward = 38.0, mean_100 = 20.5, pg_loss = 0.564, v_loss = 54.213\n",
      "[Episode   13] reward = 11.0, mean_100 = 19.8, pg_loss = -0.065, v_loss = 16.257\n",
      "[Episode   14] reward = 12.0, mean_100 = 19.3, pg_loss = -0.030, v_loss = 17.740\n",
      "[Episode   15] reward = 12.0, mean_100 = 18.8, pg_loss = -0.018, v_loss = 26.793\n",
      "[Episode   16] reward = 12.0, mean_100 = 18.4, pg_loss = -0.052, v_loss = 25.788\n",
      "[Episode   17] reward = 9.0, mean_100 = 17.9, pg_loss = 0.001, v_loss = 22.021\n",
      "[Episode   18] reward = 29.0, mean_100 = 18.5, pg_loss = -0.011, v_loss = 63.922\n",
      "[Episode   19] reward = 16.0, mean_100 = 18.4, pg_loss = -0.015, v_loss = 24.119\n",
      "[Episode   20] reward = 14.0, mean_100 = 18.1, pg_loss = -0.002, v_loss = 23.581\n",
      "[Episode   21] reward = 25.0, mean_100 = 18.5, pg_loss = -0.000, v_loss = 39.655\n",
      "[Episode   22] reward = 9.0, mean_100 = 18.0, pg_loss = -0.031, v_loss = 25.452\n",
      "[Episode   23] reward = 28.0, mean_100 = 18.5, pg_loss = -0.021, v_loss = 60.215\n",
      "[Episode   24] reward = 16.0, mean_100 = 18.4, pg_loss = -0.008, v_loss = 41.681\n",
      "[Episode   25] reward = 23.0, mean_100 = 18.5, pg_loss = -0.002, v_loss = 34.996\n",
      "[Episode   26] reward = 57.0, mean_100 = 20.0, pg_loss = 0.102, v_loss = 72.391\n",
      "[Episode   27] reward = 29.0, mean_100 = 20.3, pg_loss = -0.001, v_loss = 47.966\n",
      "[Episode   28] reward = 60.0, mean_100 = 21.7, pg_loss = 0.013, v_loss = 80.147\n",
      "[Episode   29] reward = 50.0, mean_100 = 22.6, pg_loss = -0.052, v_loss = 74.411\n",
      "\n",
      "===== Training PPO on Acrobot-v1 =====\n",
      "[Episode    0] reward = -500.0, mean_100 = -500.0, pg_loss = 0.018, v_loss = 132.698\n",
      "[Episode    1] reward = -500.0, mean_100 = -500.0, pg_loss = 0.153, v_loss = 133.673\n",
      "[Episode    2] reward = -500.0, mean_100 = -500.0, pg_loss = 0.003, v_loss = 128.547\n",
      "[Episode    3] reward = -500.0, mean_100 = -500.0, pg_loss = -0.042, v_loss = 124.985\n",
      "[Episode    4] reward = -500.0, mean_100 = -500.0, pg_loss = -0.194, v_loss = 123.408\n",
      "[Episode    5] reward = -500.0, mean_100 = -500.0, pg_loss = 0.127, v_loss = 123.670\n",
      "[Episode    6] reward = -500.0, mean_100 = -500.0, pg_loss = 0.081, v_loss = 120.142\n",
      "[Episode    7] reward = -500.0, mean_100 = -500.0, pg_loss = -0.015, v_loss = 116.705\n",
      "[Episode    8] reward = -500.0, mean_100 = -500.0, pg_loss = -0.211, v_loss = 107.817\n",
      "[Episode    9] reward = -500.0, mean_100 = -500.0, pg_loss = -0.131, v_loss = 107.417\n",
      "[Episode   10] reward = -500.0, mean_100 = -500.0, pg_loss = 0.235, v_loss = 120.288\n",
      "[Episode   11] reward = -500.0, mean_100 = -500.0, pg_loss = -0.363, v_loss = 88.530\n",
      "[Episode   12] reward = -500.0, mean_100 = -500.0, pg_loss = 0.286, v_loss = 113.505\n",
      "[Episode   13] reward = -500.0, mean_100 = -500.0, pg_loss = -0.371, v_loss = 91.202\n",
      "[Episode   14] reward = -500.0, mean_100 = -500.0, pg_loss = -0.077, v_loss = 94.998\n",
      "[Episode   15] reward = -500.0, mean_100 = -500.0, pg_loss = 0.257, v_loss = 107.760\n",
      "[Episode   16] reward = -500.0, mean_100 = -500.0, pg_loss = -0.170, v_loss = 95.702\n",
      "[Episode   17] reward = -500.0, mean_100 = -500.0, pg_loss = -0.602, v_loss = 79.396\n",
      "[Episode   18] reward = -500.0, mean_100 = -500.0, pg_loss = 0.169, v_loss = 98.990\n",
      "[Episode   19] reward = -500.0, mean_100 = -500.0, pg_loss = 0.148, v_loss = 93.066\n",
      "[Episode   20] reward = -500.0, mean_100 = -500.0, pg_loss = 0.153, v_loss = 93.275\n",
      "[Episode   21] reward = -500.0, mean_100 = -500.0, pg_loss = -0.278, v_loss = 78.585\n",
      "[Episode   22] reward = -500.0, mean_100 = -500.0, pg_loss = 0.063, v_loss = 87.374\n",
      "[Episode   23] reward = -500.0, mean_100 = -500.0, pg_loss = -0.082, v_loss = 81.143\n",
      "[Episode   24] reward = -500.0, mean_100 = -500.0, pg_loss = 0.283, v_loss = 97.613\n",
      "[Episode   25] reward = -500.0, mean_100 = -500.0, pg_loss = -0.164, v_loss = 72.809\n",
      "[Episode   26] reward = -500.0, mean_100 = -500.0, pg_loss = 0.038, v_loss = 77.960\n",
      "[Episode   27] reward = -500.0, mean_100 = -500.0, pg_loss = -0.267, v_loss = 70.164\n",
      "[Episode   28] reward = -500.0, mean_100 = -500.0, pg_loss = -0.035, v_loss = 77.521\n",
      "[Episode   29] reward = -500.0, mean_100 = -500.0, pg_loss = -0.060, v_loss = 70.957\n",
      "\n",
      "===== Training PPO on MountainCar-v0 =====\n",
      "[Episode    0] reward = -200.0, mean_100 = -200.0, pg_loss = 0.118, v_loss = 125.602\n",
      "[Episode    1] reward = -200.0, mean_100 = -200.0, pg_loss = -0.297, v_loss = 108.393\n",
      "[Episode    2] reward = -178.0, mean_100 = -192.7, pg_loss = 0.194, v_loss = 122.567\n",
      "[Episode    3] reward = -180.0, mean_100 = -189.5, pg_loss = 0.166, v_loss = 119.181\n",
      "[Episode    4] reward = -200.0, mean_100 = -191.6, pg_loss = 0.330, v_loss = 126.372\n",
      "[Episode    5] reward = -200.0, mean_100 = -193.0, pg_loss = 0.371, v_loss = 124.555\n",
      "[Episode    6] reward = -200.0, mean_100 = -194.0, pg_loss = -0.490, v_loss = 103.329\n",
      "[Episode    7] reward = -200.0, mean_100 = -194.8, pg_loss = 0.130, v_loss = 111.320\n",
      "[Episode    8] reward = -200.0, mean_100 = -195.3, pg_loss = -0.359, v_loss = 104.069\n",
      "[Episode    9] reward = -186.0, mean_100 = -194.4, pg_loss = -0.047, v_loss = 101.913\n",
      "[Episode   10] reward = -200.0, mean_100 = -194.9, pg_loss = -1.151, v_loss = 82.151\n",
      "[Episode   11] reward = -200.0, mean_100 = -195.3, pg_loss = -0.529, v_loss = 84.067\n",
      "[Episode   12] reward = -200.0, mean_100 = -195.7, pg_loss = 0.248, v_loss = 103.349\n",
      "[Episode   13] reward = -200.0, mean_100 = -196.0, pg_loss = 0.441, v_loss = 107.134\n",
      "[Episode   14] reward = -200.0, mean_100 = -196.3, pg_loss = 0.416, v_loss = 105.320\n",
      "[Episode   15] reward = -166.0, mean_100 = -194.4, pg_loss = 0.016, v_loss = 82.972\n",
      "[Episode   16] reward = -149.0, mean_100 = -191.7, pg_loss = -0.412, v_loss = 67.105\n",
      "[Episode   17] reward = -153.0, mean_100 = -189.6, pg_loss = 0.274, v_loss = 85.090\n",
      "[Episode   18] reward = -149.0, mean_100 = -187.4, pg_loss = -0.126, v_loss = 73.029\n",
      "[Episode   19] reward = -200.0, mean_100 = -188.1, pg_loss = 0.322, v_loss = 82.589\n",
      "[Episode   20] reward = -163.0, mean_100 = -186.9, pg_loss = 0.121, v_loss = 70.464\n",
      "[Episode   21] reward = -137.0, mean_100 = -184.6, pg_loss = -0.088, v_loss = 62.067\n",
      "[Episode   22] reward = -95.0, mean_100 = -180.7, pg_loss = 0.378, v_loss = 60.221\n",
      "[Episode   23] reward = -75.0, mean_100 = -176.3, pg_loss = 0.140, v_loss = 44.036\n",
      "[Episode   24] reward = -84.0, mean_100 = -172.6, pg_loss = 0.173, v_loss = 44.466\n",
      "[Episode   25] reward = -97.0, mean_100 = -169.7, pg_loss = 0.271, v_loss = 48.757\n",
      "[Episode   26] reward = -84.0, mean_100 = -166.5, pg_loss = 0.150, v_loss = 41.961\n",
      "[Episode   27] reward = -81.0, mean_100 = -163.5, pg_loss = 0.040, v_loss = 34.483\n",
      "[Episode   28] reward = -81.0, mean_100 = -160.6, pg_loss = -0.426, v_loss = 27.092\n",
      "[Episode   29] reward = -64.0, mean_100 = -157.4, pg_loss = 0.176, v_loss = 25.223\n",
      "\n",
      "===== Training PPO on MountainCarContinuous-v0 =====\n",
      "[Episode    0] reward = 56.6, mean_100 = 56.6, pg_loss = 0.293, v_loss = 1.665\n",
      "[Episode    1] reward = 12.5, mean_100 = 34.6, pg_loss = 0.125, v_loss = 3.745\n",
      "[Episode    2] reward = 90.4, mean_100 = 53.2, pg_loss = 0.345, v_loss = 64.340\n",
      "[Episode    3] reward = 85.8, mean_100 = 61.3, pg_loss = -0.008, v_loss = 417.821\n",
      "[Episode    4] reward = 81.7, mean_100 = 65.4, pg_loss = -0.051, v_loss = 180.903\n",
      "[Episode    5] reward = 76.4, mean_100 = 67.2, pg_loss = -0.235, v_loss = 343.057\n",
      "[Episode    6] reward = 86.2, mean_100 = 69.9, pg_loss = 0.459, v_loss = 0.941\n",
      "[Episode    7] reward = 86.4, mean_100 = 72.0, pg_loss = -0.028, v_loss = 293.950\n",
      "[Episode    8] reward = 88.4, mean_100 = 73.8, pg_loss = 0.002, v_loss = 469.489\n",
      "[Episode    9] reward = 82.8, mean_100 = 74.7, pg_loss = 0.219, v_loss = 118.746\n",
      "[Episode   10] reward = 89.1, mean_100 = 76.0, pg_loss = 0.509, v_loss = 0.909\n",
      "[Episode   11] reward = 90.9, mean_100 = 77.3, pg_loss = -0.023, v_loss = 673.013\n",
      "[Episode   12] reward = 84.3, mean_100 = 77.8, pg_loss = 0.338, v_loss = 71.161\n",
      "[Episode   13] reward = 83.7, mean_100 = 78.2, pg_loss = -0.166, v_loss = 734.502\n",
      "[Episode   14] reward = 86.8, mean_100 = 78.8, pg_loss = -0.148, v_loss = 599.960\n",
      "[Episode   15] reward = 81.5, mean_100 = 79.0, pg_loss = 0.192, v_loss = 375.040\n",
      "[Episode   16] reward = 89.4, mean_100 = 79.6, pg_loss = -0.327, v_loss = 1084.209\n",
      "[Episode   17] reward = 88.9, mean_100 = 80.1, pg_loss = 0.104, v_loss = 353.335\n",
      "[Episode   18] reward = 81.7, mean_100 = 80.2, pg_loss = 0.348, v_loss = 18.792\n",
      "[Episode   19] reward = 84.3, mean_100 = 80.4, pg_loss = -0.189, v_loss = 671.811\n",
      "[Episode   20] reward = 80.8, mean_100 = 80.4, pg_loss = -0.014, v_loss = 335.145\n",
      "[Episode   21] reward = 86.8, mean_100 = 80.7, pg_loss = 0.155, v_loss = 197.256\n",
      "[Episode   22] reward = 81.1, mean_100 = 80.7, pg_loss = 0.462, v_loss = 13.749\n",
      "[Episode   23] reward = 85.7, mean_100 = 80.9, pg_loss = -0.205, v_loss = 525.679\n",
      "[Episode   24] reward = 86.5, mean_100 = 81.1, pg_loss = -0.282, v_loss = 1254.051\n",
      "[Episode   25] reward = 83.0, mean_100 = 81.2, pg_loss = -0.095, v_loss = 881.878\n",
      "[Episode   26] reward = 86.9, mean_100 = 81.4, pg_loss = -0.091, v_loss = 1046.006\n",
      "[Episode   27] reward = 88.0, mean_100 = 81.7, pg_loss = 0.022, v_loss = 780.927\n",
      "[Episode   28] reward = 88.6, mean_100 = 81.9, pg_loss = 0.031, v_loss = 815.724\n",
      "[Episode   29] reward = 88.4, mean_100 = 82.1, pg_loss = 0.018, v_loss = 733.980\n",
      "\n",
      "===== Training PPO on Pendulum-v1 =====\n",
      "[Episode    0] reward = -1683.7, mean_100 = -1683.7, pg_loss = 0.191, v_loss = 9221.836\n",
      "[Episode    1] reward = -1281.6, mean_100 = -1482.7, pg_loss = -0.230, v_loss = 4626.040\n",
      "[Episode    2] reward = -1216.6, mean_100 = -1394.0, pg_loss = -0.778, v_loss = 3499.682\n",
      "[Episode    3] reward = -1159.2, mean_100 = -1335.3, pg_loss = -0.351, v_loss = 3564.540\n",
      "[Episode    4] reward = -1319.6, mean_100 = -1332.1, pg_loss = -0.104, v_loss = 5320.447\n",
      "[Episode    5] reward = -1390.8, mean_100 = -1341.9, pg_loss = -0.427, v_loss = 4994.336\n",
      "[Episode    6] reward = -1013.6, mean_100 = -1295.0, pg_loss = -0.364, v_loss = 2546.552\n",
      "[Episode    7] reward = -1140.6, mean_100 = -1275.7, pg_loss = 0.270, v_loss = 4505.986\n",
      "[Episode    8] reward = -1105.8, mean_100 = -1256.8, pg_loss = -0.317, v_loss = 3314.001\n",
      "[Episode    9] reward = -1612.8, mean_100 = -1292.4, pg_loss = -0.334, v_loss = 7308.723\n",
      "[Episode   10] reward = -1259.0, mean_100 = -1289.4, pg_loss = -0.178, v_loss = 4960.168\n",
      "[Episode   11] reward = -1251.4, mean_100 = -1286.2, pg_loss = 0.068, v_loss = 5275.960\n",
      "[Episode   12] reward = -1478.3, mean_100 = -1301.0, pg_loss = 0.413, v_loss = 7783.417\n",
      "[Episode   13] reward = -959.6, mean_100 = -1276.6, pg_loss = -0.505, v_loss = 1887.919\n",
      "[Episode   14] reward = -1016.9, mean_100 = -1259.3, pg_loss = 0.555, v_loss = 4139.640\n",
      "[Episode   15] reward = -1292.8, mean_100 = -1261.4, pg_loss = 0.191, v_loss = 5480.949\n",
      "[Episode   16] reward = -1309.4, mean_100 = -1264.2, pg_loss = 0.212, v_loss = 5855.467\n",
      "[Episode   17] reward = -1098.3, mean_100 = -1255.0, pg_loss = 0.000, v_loss = 4271.345\n",
      "[Episode   18] reward = -1378.9, mean_100 = -1261.5, pg_loss = 0.607, v_loss = 6871.436\n",
      "[Episode   19] reward = -1313.4, mean_100 = -1264.1, pg_loss = 0.174, v_loss = 5874.195\n",
      "[Episode   20] reward = -1433.5, mean_100 = -1272.2, pg_loss = 0.198, v_loss = 6721.532\n",
      "[Episode   21] reward = -1051.6, mean_100 = -1262.2, pg_loss = -0.035, v_loss = 3081.980\n",
      "[Episode   22] reward = -1035.7, mean_100 = -1252.3, pg_loss = -0.572, v_loss = 1917.569\n",
      "[Episode   23] reward = -1267.1, mean_100 = -1252.9, pg_loss = 0.126, v_loss = 4731.843\n",
      "[Episode   24] reward = -1339.6, mean_100 = -1256.4, pg_loss = 0.428, v_loss = 6243.740\n",
      "[Episode   25] reward = -1292.3, mean_100 = -1257.8, pg_loss = 0.144, v_loss = 4883.014\n",
      "[Episode   26] reward = -1364.6, mean_100 = -1261.7, pg_loss = -0.123, v_loss = 4521.971\n",
      "[Episode   27] reward = -1070.8, mean_100 = -1254.9, pg_loss = 0.306, v_loss = 4143.194\n",
      "[Episode   28] reward = -1110.5, mean_100 = -1249.9, pg_loss = -0.013, v_loss = 3437.313\n",
      "[Episode   29] reward = -1042.3, mean_100 = -1243.0, pg_loss = 0.270, v_loss = 3482.229\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'env': 'FrozenLake-v1',\n",
       "  'is_discrete_action': True,\n",
       "  's_dim': 1,\n",
       "  'a_dim': 4,\n",
       "  'best_reward': 0.0,\n",
       "  'status': 'ok',\n",
       "  'error': ''},\n",
       " {'env': 'CliffWalking-v1',\n",
       "  'is_discrete_action': True,\n",
       "  's_dim': 1,\n",
       "  'a_dim': 4,\n",
       "  'best_reward': -1190.0,\n",
       "  'status': 'ok',\n",
       "  'error': ''},\n",
       " {'env': 'CartPole-v1',\n",
       "  'is_discrete_action': True,\n",
       "  's_dim': 4,\n",
       "  'a_dim': 2,\n",
       "  'best_reward': 30.0,\n",
       "  'status': 'ok',\n",
       "  'error': ''},\n",
       " {'env': 'Acrobot-v1',\n",
       "  'is_discrete_action': True,\n",
       "  's_dim': 6,\n",
       "  'a_dim': 3,\n",
       "  'best_reward': -500.0,\n",
       "  'status': 'ok',\n",
       "  'error': ''},\n",
       " {'env': 'MountainCar-v0',\n",
       "  'is_discrete_action': True,\n",
       "  's_dim': 2,\n",
       "  'a_dim': 3,\n",
       "  'best_reward': -157.39999389648438,\n",
       "  'status': 'ok',\n",
       "  'error': ''},\n",
       " {'env': 'MountainCarContinuous-v0',\n",
       "  'is_discrete_action': False,\n",
       "  's_dim': 2,\n",
       "  'a_dim': 1,\n",
       "  'best_reward': 82.12165832519531,\n",
       "  'status': 'ok',\n",
       "  'error': ''},\n",
       " {'env': 'Pendulum-v1',\n",
       "  'is_discrete_action': False,\n",
       "  's_dim': 3,\n",
       "  'a_dim': 1,\n",
       "  'best_reward': -1243.0189208984375,\n",
       "  'status': 'ok',\n",
       "  'error': ''}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for spec in ENV_SPECS:\n",
    "    env_name = spec[\"name\"]\n",
    "    print(f\"\\n===== Training PPO on {env_name} =====\")\n",
    "\n",
    "    try:\n",
    "        out = train_single_env(spec, max_episodes=MAX_EPISODES_PER_ENV)\n",
    "        out[\"status\"] = \"ok\"\n",
    "        out[\"error\"] = \"\"\n",
    "        results.append(out)\n",
    "    except Exception as exc:\n",
    "        results.append({\n",
    "            \"env\": env_name,\n",
    "            \"is_discrete_action\": None,\n",
    "            \"s_dim\": None,\n",
    "            \"a_dim\": None,\n",
    "            \"best_reward\": None,\n",
    "            \"status\": \"failed\",\n",
    "            \"error\": f\"{type(exc).__name__}: {exc}\",\n",
    "        })\n",
    "        print(f\"FAILED: {type(exc).__name__}: {exc}\")\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7221c2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment                  ActionType   BestReward     Status  \n",
      "-----------------------------------------------------------------\n",
      "FrozenLake-v1                discrete     0.000          ok      \n",
      "CliffWalking-v1              discrete     -1190.000      ok      \n",
      "CartPole-v1                  discrete     30.000         ok      \n",
      "Acrobot-v1                   discrete     -500.000       ok      \n",
      "MountainCar-v0               discrete     -157.400       ok      \n",
      "MountainCarContinuous-v0     continuous   82.122         ok      \n",
      "Pendulum-v1                  continuous   -1243.019      ok      \n"
     ]
    }
   ],
   "source": [
    "def print_results_table(rows):\n",
    "    header = f\"{'Environment':<28} {'ActionType':<12} {'BestReward':<14} {'Status':<8}\"\n",
    "    print(header)\n",
    "    print('-' * len(header))\n",
    "\n",
    "    for row in rows:\n",
    "        action_type = (\"discrete\" if row[\"is_discrete_action\"] else \"continuous\") if row[\"is_discrete_action\"] is not None else \"-\"\n",
    "        reward = f\"{row['best_reward']:.3f}\" if row[\"best_reward\"] is not None else \"-\"\n",
    "        print(f\"{row['env']:<28} {action_type:<12} {reward:<14} {row['status']:<8}\")\n",
    "\n",
    "\n",
    "print_results_table(results)\n",
    "\n",
    "failed = [row for row in results if row['status'] != 'ok']\n",
    "if failed:\n",
    "    print('\\nFailures:')\n",
    "    for row in failed:\n",
    "        print(f\"- {row['env']}: {row['error']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919162c4",
   "metadata": {},
   "source": [
    "Some experiment failed miserably, but some is doing somewhat well. This is because PPO does not account for non-stationarity. Changes in the environment kills the prior belief in the algorithm.\n",
    "\n",
    "Particularly, FrozenLake did exceptionally bad. Here is GPT's response to this:\n",
    "1. FrozenLake reward is extremely sparse (only rewarded 1 when reaches the end)\n",
    "2. Non-stationarity is too strong.\n",
    "3. PPO does not use non-stationarity signals"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nsgym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
